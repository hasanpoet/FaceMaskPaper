%%%%%%%%%%%%%%%%%%%% author.tex %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% sample root file for your "contribution" to a proceedings volume
%
% Use this file as a template for your own input.
%
%%%%%%%%%%%%%%%% Springer %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass{svproc}
%
% RECOMMENDED %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%

% to typeset URLs, URIs, and DOIs
\usepackage{url}
\usepackage{float}
\def\UrlFont{\rmfamily}

\begin{document}
\mainmatter              % start of a contribution
%
\title{A Deep Learning approach for detecting medical face mask on human faces in response to Covid19}
%
\titlerunning{Medical Face Mask Detection}  % abbreviated title (for running head)
%                                     also used for the TOC unless
%                                     \toctitle is used
%
\author{Borun Das\inst{1} \and Mahmud Hasan\inst{2}}
%
\authorrunning{Borun Das et al.} % abbreviated author list (for running head)
%
%%%% list of authors for the TOC (use if author list has to be modified)
\tocauthor{Borun Das, Mahmud Hasan}
%
\institute{Learning Imaging Lab, Dhaka, Bangladesh\\
\email{nilborun@gmail.com},
\and
University of Western Ontario, ON, Canada\\
\email{mhasan62@uwo.ca}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
In this difficult time of COVID19, using a facial mask is a life saver, specially in all indoor public places. In this work, a deep learning based approach is proposed that detects whether or not a human face contains a face mask. The proposed method is capable of detecting the facial mask with $98\%$ accuracy for any frontal face static image or videos. The validation was performed on a variety of different scenarios to ensure the accuracy.
% We would like to encourage you to list your keywords within
% the abstract section using the \keywords{...} command.
\keywords{deep learning, face mask detection, Covid19}
\end{abstract}
%
\section{Background}
%
COVID19 pandemic hit the world hard in the year 2020, both economically and medically. On one side, we discovered that human beings are so vulnerable against mother nature even after having so many advancement in science and technology. On the other side, we also found ourselves in extreme financial difficulties as businesses around the world has been suffering due to lack of customers and demands. Although a few vaccine candidates were developed, tested and started mass vaccination towards the end of the year, scientists are yet to confirm the overall efficiency, side effects, time the created antobodies are effective for and so on. Even if the vaccines work perfectly, it will take years to develop billions of doses and vaccinate people around the world. And until that, personal protective equipment (PPE) that the front line workers and people have been using is our lifeguard. World Health Organization (WHO) advised that among all of the PPEs available, face masks are the most protective as they directly cover nose and mouth - two most susceptible areas of human body. Another vulnerable area is the eyes, and WHO also suggested to use safety goggles, specially for the health service workers. For general people, however, the face mask at the minimum. \\

\par Now although people around the world are advised to use face mask in all indoor places and recommended in outdoor, it was not very uncommon to see that many people are ignoring the face mask or not properly using them, putting all of us at risk. We have also seen movement against using face masks that forced many countries/authorities to enforce the usage of face masks in indoor public spaces. Authorities/officials were also seen using thermal detector to detect people's temperature before letting them enter any public space. While using a thermal gate at the entry point was easy to make this work (semi) automated, checking whether or not every person was using face mask was not that easy, due to unavailability of related technology. The automated technologies that authorities have been using around the world are mostly about detecting face, not face masks. So, this had to be done mostly manually, and enforcement became difficult causing more and more positive cases over the time almost everywhere. \\

In this work, we came up with a deep learning based approach to automatically detect whether or not a person is using face mask from both static and video images. The outcome can be used behind any camera currently in effect to detect if the person entering a space is using the face mask. It is similar to the thermal gate installed to detect the temperature and notify the authority if temperature is above a certain threshold for an subject (human). In similar manner, our proposed work is able to detect and notify the authority if the subject is not wearing the face mask properly, making the enforcement and warning easier. The proposed method was properly tested to verify that for any frontal face image/video, it is able to detect \emph{mask/no mask} correctly. The performance analysis showed that for any situation (out of 9 different testings performed) the proposed method is able to achieve $98\%$ accuracy.   

\section{Related Works}

Detection of facial mask is a comparatively new area of work. Before this pandemic started, there was almost no need to pay attention to this area. As a result, although we get thousands of works related to face detection or facial expression detection, we don't get much works related to face mask detection. In 2017, Ge et al. \cite{ge2017detecting} first worked on in this area.  In 2020, there have been a few publications indicating that people started focusing on this problem. \\

\par Loey et al. \cite{loey2020fighting} proposed a novel deep learning model based on YOLO-v2 and ResNet-50 for detecting medical masks from human face. They used two publicly available data set called \emph{MMD} and \emph{FMD} in their work. They were able to achieve $81\%$ accuracy as compared to Ge at el. who achieved around $76\%$ of accuracy. Loey et al. also proposed another hybrid model \cite{loey2020hybrid} where they used three different data sets such as Real World Masked Face Data set (RMFD), Simulated Masked Face Data set (SMFD) and Labeled Faces in the Wild (LFW). While for the first two data sets they were able to achieve only around $94\%$ and $96\%$ respectively, for the third data set they were able to achieve $98\%$ accuracy. \\

\par Mohan at el. lately proposed another tiny DeepNet model for facial mask detection for resource constrained endpoints \cite{mohan2020tiny}. They used the Kaggle data set and reported a $99.81\%$ accuracy in their proposed model. However, it's worth noting that the Kaggle data set has only $440$ images that contains equal number of mask and non-mask images. Chen at el. proposed a transfer learning technique to classify the mask/non-mask images, however they did it on a semi-synthetic data set.Trying on AlexNet and VGG16, they reported a validation accuracy of $ 96\%$ and $99\%$, respectively \cite{chen2020efficient}. The last reference that we found related is by Chowdary at el. \cite{chowdary2020face}. They only used SMFD data set mentioned above and reported $100\%$ accuracy. \\

\par Given all the  related works we found that there is no work that combine all the available data sets together and observe the responses, most of the works are focused on one or two data sets. So the accuracy reported by them are limited. Also, only one of the papers reported their training time, so finding a faster learning model is yet to be achieved. No previous work was found so far that also tested their method on video, therefore building a system that equally works for both video and still pictures is in scope, opening up the real application of the system behind the CCTV/Security Camera is a large indoor public space. 

\section{Proposed Method}

The model we are proposing is outlined in table \ref{modelTable}.

\begin{table}[]
\centering
\begin{tabular}{l}
Model: "sequential"                                                                                                                \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
Layer (type)                 Output Shape              Param \#                                                                    \\
===============================                                                   \\
conv2d (Conv2D)              (None, 148, 148, 32)      896                                                                         \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
max\_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0                                                                          \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
conv2d\_1 (Conv2D)            (None, 72, 72, 32)        9248                                                                       \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
max\_pooling2d\_1 (MaxPooling2 (None, 36, 36, 32)        0                                                                         \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
conv2d\_2 (Conv2D)            (None, 34, 34, 64)        18496                                                                      \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
max\_pooling2d\_2 (MaxPooling2 (None, 17, 17, 64)        0                                                                         \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
flatten (Flatten)            (None, 18496)             0                                                                           \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
dense (Dense)                (None, 64)                1183808                                                                     \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
activation (Activation)      (None, 64)                0                                                                           \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
dropout (Dropout)            (None, 64)                0                                                                           \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
dense\_1 (Dense)              (None, 1)                 65                                                                         \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
activation\_1 (Activation)    (None, 1)                 0                                                                          \\
===============================                                                   \\
Total params: 1,212,513                                                                                                            \\
Trainable params: 1,212,513                                                                                                        \\
Non-trainable params: 0                                                                                                            \\
\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ \\
\end{tabular}
\caption{The description of the proposed model}
\label{modelTable}
\end{table}

\section{Results}
\subsection{Limitations}
\subsection{Future Works}
\section{Discussion}

%
% ---- Bibliography ----
%
\begin{thebibliography}{6}
%
\bibitem {ge2017detecting}
Ge, S., Li, J., Ye, Q. and Luo, Z., 2017. Detecting masked faces in the wild with lle-cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2682-2690).

\bibitem {loey2020fighting}
Loey, M., Manogaran, G., Taha, M.H.N. and Khalifa, N.E.M., 2020. Fighting against COVID-19: A novel deep learning model based on YOLO-v2 with ResNet-50 for medical face mask detection. Sustainable Cities and Society, p.102600.

\bibitem {loey2020hybrid}
Loey, M., Manogaran, G., Taha, M.H.N. and Khalifa, N.E.M., 2020. A hybrid deep transfer learning model with machine learning methods for face mask detection in the era of the COVID-19 pandemic. Measurement, 167, p.108288.

\bibitem {mohan2020tiny}
Mohan, P., Paul, A.J. and Chirania, A., 2020. A Tiny CNN Architecture for Medical Face Mask Detection for Resource-Constrained Endpoints. arXiv preprint arXiv:2011.14858.

\bibitem {chen2020efficient}
Chen, S., Liu, W. and Zhang, G., 2020. Efficient Transfer Learning Combined Skip-Connected Structure for Masked Face Poses Classification. IEEE Access, 8, pp.209688-209698.

\bibitem {chowdary2020face}
Chowdary, G.J., Punn, N.S., Sonbhadra, S.K. and Agarwal, S., 2020. Face Mask Detection using Transfer Learning of InceptionV3. arXiv preprint arXiv:2009.08369.

\bibitem {ourVideo}
A Test Video by the Authors. \url{http://www.youtube.com/}

\end{thebibliography}
\end{document}
